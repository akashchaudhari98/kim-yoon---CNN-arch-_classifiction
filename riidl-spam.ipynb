{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import csv\n",
    "import os\n",
    "os.chdir(\"C:/New/spam\")\n",
    "with open('emaildata1.csv' , 'r' ) as emaildata:\n",
    "    email_reader = csv.reader(emaildata)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"C:/New/spam/train.csv\")\n",
    "#print (train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "got all labels(0,1)\n",
    "matched the labels with the email text sr.no\n",
    "total email - 2181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "{0: 0, 1: 1}\n",
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "5          0\n",
      "6          0\n",
      "7          0\n",
      "8          0\n",
      "9          0\n",
      "10         0\n",
      "11         0\n",
      "12         0\n",
      "13         0\n",
      "14         0\n",
      "15         0\n",
      "16         0\n",
      "17         0\n",
      "18         0\n",
      "19         0\n",
      "20         0\n",
      "21         0\n",
      "22         1\n",
      "23         0\n",
      "24         0\n",
      "25         0\n",
      "26         0\n",
      "27         0\n",
      "28         0\n",
      "29         0\n",
      "          ..\n",
      "1048545    0\n",
      "1048546    0\n",
      "1048547    0\n",
      "1048548    0\n",
      "1048549    0\n",
      "1048550    0\n",
      "1048551    0\n",
      "1048552    0\n",
      "1048553    0\n",
      "1048554    0\n",
      "1048555    0\n",
      "1048556    0\n",
      "1048557    1\n",
      "1048558    0\n",
      "1048559    0\n",
      "1048560    0\n",
      "1048561    0\n",
      "1048562    0\n",
      "1048563    0\n",
      "1048564    0\n",
      "1048565    0\n",
      "1048566    0\n",
      "1048567    0\n",
      "1048568    0\n",
      "1048569    0\n",
      "1048570    0\n",
      "1048571    0\n",
      "1048572    0\n",
      "1048573    0\n",
      "1048574    0\n",
      "Name: label, Length: 1048575, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label = train_data.label.unique()\n",
    "print(label)\n",
    "#.unique gets all the unique values from the list\n",
    "dic = {}\n",
    "for i,label in enumerate(label):\n",
    "    dic[label] = i\n",
    "labels = train_data.label.apply(lambda x:dic[x]) #gave a numeric label to each email\n",
    "y_train ={}\n",
    "   \n",
    "print(dic)\n",
    "#type(y_train)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized the emails (texts) using keras tokenizer\n",
    "word index - a unqiue integer is given to every word\n",
    "sequence_train is all the email data represented in integers \n",
    "total words in the texts is 40162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "texts = train_data.texts #selecting the email data from the csv file\n",
    "#print(texts) #texts is just the email data\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=40162,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts)#converting the emails to sequences\n",
    "#print(sequence_train)\n",
    "\n",
    "word_index = tokenizer.word_index #word index is numbers given to indivisual words in data\n",
    "#rint(word_index)\n",
    "print((word_index))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padded the email ,so that all the emails are of the same size when a matrix is created of there word indexes\n",
    "x_train is the padded array of all email data in integer form ..shape(2182,3590)\n",
    "y_train is lables array , it has a length of total emails and width of 2 bits - 10/01(0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1048575, 191)\n",
      "[[   0    0    0 ...    6    1 8069]\n",
      " [   0    0    0 ...   10   46 1830]\n",
      " [   0    0    0 ...  380  458 5511]\n",
      " ...\n",
      " [   0    0    0 ...   27 6548 8747]\n",
      " [   0    0    0 ... 2120 2875 1840]\n",
      " [   0    0    0 ...  424  393    7]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "(1048575, 2)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "print(\"1\")\n",
    "#padding the sequence train , ie making all the matrix same\n",
    "x_train = pad_sequences(sequence_train)\n",
    "print(x_train.shape)\n",
    "print(x_train)\n",
    "#print(\"2\")\n",
    "print(np.asarray(labels))#converts all label values of email into a matrix\n",
    "y_train = to_categorical(np.asarray(labels)) #cheak out doc example \n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "#print(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "word2vec_model300  = gensim.models.KeyedVectors.load_word2vec_format(\"D:\\Dataset\\word2vec-GoogleNews-vectors-master\\GoogleNews-vectors-negative300.bin\",binary= True)\n",
    "#print(word2vec_model300 )\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181996\n",
      "(181996, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "num_word = 40162\n",
    "vocab_size = len(word_index)\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim)) #defining size of the embedding matrix\n",
    "print(embedding_matrix.shape)\n",
    "for word,i in word_index.items(): \n",
    "    #print(word)\n",
    "    if i>=40162:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector =word2vec_model300[word]#getting the word vector for the words\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "         embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),300)\n",
    "        #length of all the embedding matrix is 300\n",
    "print(embedding_matrix[3].shape) #getting a matrix for every word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x000002126869AAC8>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(embedding_matrix)\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "embedding_layer = Embedding(vocab_size,embedding_dim,embeddings_initializer=Constant(embedding_matrix),trainable=True  )\n",
    "print(embedding_layer)\n",
    "#embeddings_initializer=Constant(embedding_matrix),\n",
    "#embedding_layer.input()1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n",
      "Done\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "sequence_length = x_train.shape[1]\n",
    "#basically length of the first email (after padding ,so all email shapes are same)\n",
    "print(x_train[1].shape)\n",
    "#print(sequence_length)\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "\n",
    "drop = 0.5\n",
    "\n",
    "print(\"Done\")\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input layer\n",
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
      "WARNING:tensorflow:From C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "embedding layer\n",
      "Tensor(\"Shape_1:0\", shape=(3,), dtype=int32)\n",
      "reshape layer\n",
      "Tensor(\"Shape_2:0\", shape=(4,), dtype=int32)\n",
      "conv0 layer\n",
      "Tensor(\"Shape_3:0\", shape=(4,), dtype=int32)\n",
      "conv1 layer\n",
      "Tensor(\"Shape_4:0\", shape=(4,), dtype=int32)\n",
      "conv2 layer\n",
      "Tensor(\"Shape_5:0\", shape=(4,), dtype=int32)\n",
      "maxpool0 layer\n",
      "Tensor(\"Shape_6:0\", shape=(4,), dtype=int32)\n",
      "maxpool1 layer\n",
      "Tensor(\"Shape_7:0\", shape=(4,), dtype=int32)\n",
      "maxpool2 layer\n",
      "Tensor(\"Shape_8:0\", shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(sequence_length,))  \n",
    "print(\"input layer\")\n",
    "print(keras.backend.shape(inputs))\n",
    "#embedding_layer = Embedding(vocab_size, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable=False)(inputs)\n",
    "embedding = embedding_layer(inputs)\n",
    "print(\"embedding layer\")\n",
    "print(keras.backend.shape(embedding))\n",
    "\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "print(\"reshape layer\")\n",
    "print(keras.backend.shape(reshape))\n",
    "\n",
    "#conv_0 = Conv2D(num_filters, (filter_sizes[0], embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_0 = Conv2D(512, (filter_sizes[0], embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "print(\"conv0 layer\")\n",
    "print(keras.backend.shape(conv_0))\n",
    "\n",
    "conv_1 = Conv2D(512, (filter_sizes[1],embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "print('conv1 layer')\n",
    "print(keras.backend.shape(conv_1))\n",
    "\n",
    "conv_2 = Conv2D(512, (filter_sizes[2],embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "print(\"conv2 layer\")\n",
    "print(keras.backend.shape(conv_2))\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "print(\"maxpool0 layer\")\n",
    "print(keras.backend.shape(maxpool_0))\n",
    "#flat_0 = Flatten()(maxpool_0)\n",
    "\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "print(\"maxpool1 layer\")\n",
    "print(keras.backend.shape(maxpool_1))\n",
    "#flat_1 = Flatten()(maxpool_1)\n",
    "\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "print(\"maxpool2 layer\")\n",
    "print(keras.backend.shape(maxpool_2))\n",
    "#flat_2 = Flatten()(maxpool_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_9:0\", shape=(4,), dtype=int32)\n",
      "Tensor(\"Shape_10:0\", shape=(2,), dtype=int32)\n",
      "WARNING:tensorflow:From C:\\Users\\Akash\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"Shape_11:0\", shape=(2,), dtype=int32)\n",
      "done\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Tensor(\"dense_1/Softmax:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "merged_tensor = concatenate([maxpool_0,maxpool_1, maxpool_2])\n",
    "print(keras.backend.shape(merged_tensor))\n",
    "\n",
    "flatten = Flatten()(merged_tensor)\n",
    "print(keras.backend.shape(flatten))\n",
    "\n",
    "#reshape = Reshape((3*num_filters,))(merged_tensor)\n",
    "\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "print(keras.backend.shape(output))\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n",
    "print(\"done\")\n",
    "print(y_train)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 191)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 191, 300)     54598800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 191, 300, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 189, 1, 512)  461312      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 188, 1, 512)  614912      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 187, 1, 512)  768512      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1, 1536)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1536)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            4611        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 56,448,147\n",
      "Trainable params: 56,448,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adam =Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  optimizer=adam, metrics=['acc'])\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss')\n",
    "batch_size = 3590\n",
    "print(model.summary())\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='D:\\model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (3,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-305956493474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m           \u001b[1;31m# starts training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (3,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train, batch_size  ,epochs=100, verbose=1, callbacks = callback )\n",
    "          # starts training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
